{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual coursework --Data Mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. read file from html and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name _htmlparser",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ea78d9f8a812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# read folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdammit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnicodeDammit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m from .element import (\n",
      "\u001b[0;32m/anaconda3/lib/python2.7/site-packages/bs4/builder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;31m# to take precedence over html5lib, because it's faster. And we only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;31m# want to use HTMLParser as a last result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_htmlparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0mregister_treebuilders_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_htmlparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name _htmlparser"
     ]
    }
   ],
   "source": [
    "# read html file\n",
    "from __future__ import division, unicode_literals \n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# read folder\n",
    "import codecs\n",
    "import glob\n",
    "\n",
    "# process text\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "# save to txt\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  kinetiz\n",
    "import nltk\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import manifold\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.lines as mlines\n",
    "import pickle\n",
    "\n",
    "#doc2vec\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "# lxy1992\n",
    "# from pattern.en import lemma\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# import enchant\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import MDS\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import codecs\n",
    "import csv\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import ward, dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "# import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    #Convert text to lowercase\n",
    "#     text = text.lower()\n",
    "    # remove whitespaces\n",
    "    text = text.strip()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    #Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    #remove punctuation\n",
    "#     text = text.translate(str.maketrans('', '', punctuation))\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \" \", text)\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 2. read html and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 books-24/gap-html/gap_udEIAAAAQAAJ\n",
      "2 books-24/gap-html/gap_TgpMAAAAYAAJ\n",
      "3 books-24/gap-html/gap_IlUMAQAAMAAJ\n",
      "4 books-24/gap-html/gap_CSEUAAAAYAAJ\n",
      "5 books-24/gap-html/gap_VPENAAAAQAAJ\n",
      "6 books-24/gap-html/gap_XmqHlMECi6kC\n",
      "7 books-24/gap-html/gap_CnnUAAAAMAAJ\n",
      "8 books-24/gap-html/gap_GIt0HMhqjRgC\n",
      "9 books-24/gap-html/gap_ogsNAAAAIAAJ\n",
      "10 books-24/gap-html/gap_MEoWAAAAYAAJ\n",
      "11 books-24/gap-html/gap_DqQNAAAAYAAJ\n",
      "12 books-24/gap-html/gap_aLcWAAAAQAAJ\n",
      "13 books-24/gap-html/gap_DhULAAAAYAAJ\n",
      "14 books-24/gap-html/gap_pX5KAAAAYAAJ\n",
      "15 books-24/gap-html/gap_9ksIAAAAQAAJ\n",
      "16 books-24/gap-html/gap_WORMAAAAYAAJ\n",
      "17 books-24/gap-html/gap_y-AvAAAAYAAJ\n",
      "18 books-24/gap-html/gap_-C0BAAAAQAAJ\n",
      "19 books-24/gap-html/gap_RqMNAAAAYAAJ\n",
      "20 books-24/gap-html/gap_fnAMAAAAYAAJ\n",
      "21 books-24/gap-html/gap_2X5KAAAAYAAJ\n",
      "22 books-24/gap-html/gap_m_6B1DkImIoC\n",
      "23 books-24/gap-html/gap_Bdw_AAAAYAAJ\n",
      "24 books-24/gap-html/gap_dIkBAAAAQAAJ\n"
     ]
    }
   ],
   "source": [
    "book_id = 0\n",
    "\n",
    "book_dir = \"books-24/\"\n",
    "file_dir = \"gap-html/\"\n",
    "\n",
    "files_df = pd.DataFrame(columns=['folder_name', 'contents'])\n",
    "books = pd.DataFrame(columns=['contents'])\n",
    "\n",
    "for foldername in glob.glob('gap-html/*'):\n",
    "    book_id += 1\n",
    "    file_cotents = \"\"\n",
    "    print(book_id, book_dir + foldername)\n",
    "    for filename in glob.glob(foldername+'/*.html'):\n",
    "#         print(filename)\n",
    "        file = codecs.open(filename, 'r', 'utf-8')\n",
    "#         text_data = BeautifulSoup(file.read(), 'lxml').get_text()\n",
    "#       contents = process_text(contents)\n",
    "        soup = BeautifulSoup(file.read(),\"html.parser\")\n",
    "#     text_data = BeautifulSoup(open(file),\"html.parser\")\n",
    "        ocr_tags = soup.select(\".ocr_cinfo\") #read tags\n",
    "        text_list = [tag.get_text() for tag in ocr_tags] #extract text\n",
    "        text_data = ' '.join(text_list)\n",
    "#     text = process_text(text)\n",
    "#         if text_data.strip() != '':\n",
    "        file_cotents = file_cotents + text_data + ' '\n",
    "    \n",
    "    files_df.loc[book_id] = [foldername, file_cotents]\n",
    "    books.loc[1] = [file_cotents]\n",
    "    books.to_csv(book_dir + foldername.split('/')[1] + '.csv')\n",
    "    books.to_csv(book_dir + foldername.split('/')[1] + '.txt',index=False, header = False)\n",
    "#     books.drop(index = 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "data = files_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3 preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    #initiate tokenizer and capture only English letters\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    stemmer = PorterStemmer()\n",
    "    #lower and tokenize \n",
    "#     text.\n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemma(text):\n",
    "    #initiate tokenizer and capture only English letters\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "#      stemmer = PorterStemmer()\n",
    "    #lower and tokenize \n",
    "#     text.\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessWords(words):\n",
    "    # Split words by all non alphabet character\n",
    "    words = re.compile(r'[^A-Z^a-z]+').split(words)\n",
    "    # Convert to lowercase\n",
    "    return [word for word in words if word != '']\n",
    "\n",
    "def lowerword(words):\n",
    "    return [word.lower() for word in words if 1 == 1]\n",
    "\n",
    "def removestopword(words):\n",
    "    # remove the stop words\n",
    "    stop = stopwords.words('english')\n",
    "    return [word for word in words if word not in stop]\n",
    "\n",
    "def removeshort(words):\n",
    "    # remove short word\n",
    "    return [word for word in words if len(word) > 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemma(text):\n",
    "     c = preprocessWords(text)\n",
    "    # remove short words\n",
    "    c = removeshort(c)\n",
    "    # remove misspelling words\n",
    "    c = correctword(c)\n",
    "    # remove stopwords\n",
    "    c = removestopword(c)\n",
    "    # remove short words\n",
    "#     c = lemmatization(c)\n",
    "    # remove short words again\n",
    "    c = removeshort(c)\n",
    "    # lower words\n",
    "    c = lowerword(c)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all files into one parameters\n",
    "path = \"books-24/\" \n",
    "fold = os.listdir(path)\n",
    "content = [] \n",
    "for filename in fold:\n",
    "    c = ''\n",
    "    file_path = path+\"/\"+filename\n",
    "    file = open(file_path, 'r', encoding='windows-1252')\n",
    "    c += file.read().strip()\n",
    "    content.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_data = ''.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower and tokenize\n",
    "tokens = tokenize_and_stem(str_data)\n",
    "\n",
    "#remove stop words \n",
    "eng_stopwords = stopwords.words('english') \n",
    "tokens_no_stop = [word for word in tokens if word not in eng_stopwords] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower and tokenize\n",
    "tokens2 = tokenize_and_lemma(str_data)\n",
    "\n",
    "#remove stop words \n",
    "eng_stopwords2 = stopwords.words('english') \n",
    "tokens_no_stop2 = [word for word in tokens2 if word not in eng_stopwords2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stemer=tokens_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lemma=tokens_no_stop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "teken_long = removeshort(token_stemer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "teken_long2 = removeshort(token_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roman',\n",
       " 'empir',\n",
       " 'rest',\n",
       " 'roman',\n",
       " 'empir',\n",
       " 'bpotism',\n",
       " 'princ',\n",
       " 'might',\n",
       " 'erad',\n",
       " 'silenc',\n",
       " 'sectari',\n",
       " 'obnoxi',\n",
       " 'creed',\n",
       " 'stubborn',\n",
       " 'temper',\n",
       " 'egyptian',\n",
       " 'maintain',\n",
       " 'posit',\n",
       " 'synod',\n",
       " 'chalcedon',\n",
       " 'polici',\n",
       " 'justinian',\n",
       " 'condescend',\n",
       " 'expect',\n",
       " 'seiz',\n",
       " 'portun',\n",
       " 'discord',\n",
       " 'monophysit',\n",
       " 'church',\n",
       " 'alex',\n",
       " 'andriaf',\n",
       " 'torn',\n",
       " 'disput',\n",
       " 'corrupt',\n",
       " 'incorrupt',\n",
       " 'death',\n",
       " 'patriarch',\n",
       " 'atri',\n",
       " 'action',\n",
       " 'upheld',\n",
       " 'respect',\n",
       " 'candid',\n",
       " 'aiehtheo',\n",
       " 'gaian',\n",
       " 'discipl',\n",
       " 'julian',\n",
       " 'theodosiu',\n",
       " 'pupil',\n",
       " 'severu',\n",
       " 'claim',\n",
       " 'former',\n",
       " 'support',\n",
       " 'consent',\n",
       " 'monk',\n",
       " 'senat',\n",
       " 'citi',\n",
       " 'provinc',\n",
       " 'latter',\n",
       " 'depend',\n",
       " 'prioriti',\n",
       " 'ordin',\n",
       " 'favour',\n",
       " 'empress',\n",
       " 'theodora',\n",
       " 'eunuch',\n",
       " 'nars',\n",
       " 'might',\n",
       " 'honour',\n",
       " 'warfar',\n",
       " 'exil',\n",
       " 'popular',\n",
       " 'candi',\n",
       " 'date',\n",
       " 'carthag',\n",
       " 'sardinia',\n",
       " 'inflam',\n",
       " 'ferment',\n",
       " 'alexandria',\n",
       " 'schism',\n",
       " 'hundr',\n",
       " 'seventi',\n",
       " 'year',\n",
       " 'gaianit',\n",
       " 'still',\n",
       " 'rever',\n",
       " 'memori',\n",
       " 'doctrin',\n",
       " 'founder',\n",
       " 'strength',\n",
       " 'number',\n",
       " 'disciplin',\n",
       " 'desper',\n",
       " 'bloodi',\n",
       " 'conflict',\n",
       " 'street',\n",
       " 'fill',\n",
       " 'dead',\n",
       " 'bodi',\n",
       " 'citizen',\n",
       " 'soldier',\n",
       " 'piou',\n",
       " 'women',\n",
       " 'ascend',\n",
       " 'roof',\n",
       " 'hous',\n",
       " 'shower',\n",
       " 'everi',\n",
       " 'sharp',\n",
       " 'derou',\n",
       " 'utensil',\n",
       " 'head',\n",
       " 'enemi',\n",
       " 'final',\n",
       " 'victori',\n",
       " 'nars',\n",
       " 'flame',\n",
       " 'wast',\n",
       " 'third',\n",
       " 'capit',\n",
       " 'roman',\n",
       " 'world',\n",
       " 'lieuten',\n",
       " 'justinian',\n",
       " 'conquer',\n",
       " 'caus',\n",
       " 'heret',\n",
       " 'theodosiu',\n",
       " 'speedili',\n",
       " 'though',\n",
       " 'gentli',\n",
       " 'remov',\n",
       " 'paul',\n",
       " 'tani',\n",
       " 'thodox',\n",
       " 'monk',\n",
       " 'rais',\n",
       " 'throne',\n",
       " 'athanasiu',\n",
       " 'nicr',\n",
       " 'torn',\n",
       " 'rambl',\n",
       " 'jewellar',\n",
       " 'tpho',\n",
       " 'read',\n",
       " 'noth',\n",
       " 'seen',\n",
       " 'much',\n",
       " 'well',\n",
       " 'histori',\n",
       " 'alexandrian',\n",
       " 'patriarch',\n",
       " 'dioscoru',\n",
       " 'benjamin',\n",
       " 'taken',\n",
       " 'renaudot',\n",
       " 'second',\n",
       " 'tome',\n",
       " 'annal',\n",
       " 'kutyrhin',\n",
       " 'liber',\n",
       " 'brev',\n",
       " 'victor',\n",
       " 'cbron',\n",
       " 'procop',\n",
       " 'anecdot',\n",
       " 'chap',\n",
       " 'said',\n",
       " 'gener',\n",
       " 'tent',\n",
       " 'tent',\n",
       " 'station',\n",
       " 'gener',\n",
       " 'moslem',\n",
       " 'abdallah',\n",
       " 'present',\n",
       " 'blush',\n",
       " 'import',\n",
       " 'life',\n",
       " 'temptat',\n",
       " 'held',\n",
       " 'forth',\n",
       " 'roman',\n",
       " 'prefect',\n",
       " 'retort',\n",
       " 'said',\n",
       " 'zobeir',\n",
       " 'infidel',\n",
       " 'gener',\n",
       " 'attempt',\n",
       " 'proclaim',\n",
       " 'rank',\n",
       " 'head',\n",
       " 'gregori',\n",
       " 'shall',\n",
       " 'repaid',\n",
       " 'captiv',\n",
       " 'daughter',\n",
       " 'equal',\n",
       " 'hundr',\n",
       " 'thousand',\n",
       " 'piec',\n",
       " 'gold',\n",
       " 'courag',\n",
       " 'discret',\n",
       " 'zobeir',\n",
       " 'lieuten',\n",
       " 'caliph',\n",
       " 'intrust',\n",
       " 'execut',\n",
       " 'stratagem',\n",
       " 'inclin',\n",
       " 'long',\n",
       " 'disput',\n",
       " 'balanc',\n",
       " 'favour',\n",
       " 'saracen',\n",
       " 'suppli',\n",
       " 'tiviti',\n",
       " 'artific',\n",
       " 'defici',\n",
       " 'number',\n",
       " 'part',\n",
       " 'forc',\n",
       " 'conceal',\n",
       " 'tent',\n",
       " 'mainder',\n",
       " 'prolong',\n",
       " 'irregular',\n",
       " 'skirmish',\n",
       " 'enemi',\n",
       " 'till',\n",
       " 'high',\n",
       " 'heaven',\n",
       " 'side',\n",
       " 'retir',\n",
       " 'faint',\n",
       " 'step',\n",
       " 'hors',\n",
       " 'unbridl',\n",
       " 'armour',\n",
       " 'laid',\n",
       " 'asid',\n",
       " 'hostil',\n",
       " 'nation',\n",
       " 'pare',\n",
       " 'seem',\n",
       " 'prepar',\n",
       " 'refresh',\n",
       " 'even',\n",
       " 'encount',\n",
       " 'ensu',\n",
       " 'sudden',\n",
       " 'charg',\n",
       " 'sound',\n",
       " 'arabian',\n",
       " 'camp',\n",
       " 'pour',\n",
       " 'forth',\n",
       " 'swarm',\n",
       " 'fresh',\n",
       " 'intrepid',\n",
       " 'warrior',\n",
       " 'long',\n",
       " 'line',\n",
       " 'greek',\n",
       " 'african',\n",
       " 'prise',\n",
       " 'assault',\n",
       " 'overturn',\n",
       " 'squadron',\n",
       " 'faith',\n",
       " 'fanatic',\n",
       " 'might',\n",
       " 'appear',\n",
       " 'band',\n",
       " 'angel',\n",
       " 'descend',\n",
       " 'prefect',\n",
       " 'slain',\n",
       " 'hand',\n",
       " 'zobeir',\n",
       " 'daughter',\n",
       " 'sought',\n",
       " 'reveng',\n",
       " 'death',\n",
       " 'surround',\n",
       " 'made',\n",
       " 'prison',\n",
       " 'fugit',\n",
       " 'involv',\n",
       " 'disast',\n",
       " 'town',\n",
       " 'sufetula',\n",
       " 'escap',\n",
       " 'sabr',\n",
       " 'lanc',\n",
       " 'arab',\n",
       " 'sufetula',\n",
       " 'built',\n",
       " 'hundr',\n",
       " 'fifti',\n",
       " 'mile',\n",
       " 'south',\n",
       " 'carthag',\n",
       " 'gentl',\n",
       " 'decliv',\n",
       " 'water',\n",
       " 'stream',\n",
       " 'shade',\n",
       " 'grove',\n",
       " 'junip',\n",
       " 'tree',\n",
       " 'ruin',\n",
       " 'triumphal',\n",
       " 'arch',\n",
       " 'portico',\n",
       " 'three',\n",
       " 'templ',\n",
       " 'rinthian',\n",
       " 'order',\n",
       " 'curios',\n",
       " 'admir',\n",
       " 'magnific',\n",
       " 'chap',\n",
       " 'xlvii',\n",
       " 'founder',\n",
       " 'abject',\n",
       " 'state',\n",
       " 'ignor',\n",
       " 'poverti',\n",
       " 'servitud',\n",
       " 'nestorian',\n",
       " 'monophysit',\n",
       " 'reject',\n",
       " 'spiritu',\n",
       " 'supremaci',\n",
       " 'rome',\n",
       " 'cherish',\n",
       " 'tole',\n",
       " 'ration',\n",
       " 'turkish',\n",
       " 'master',\n",
       " 'allow',\n",
       " 'anathemat',\n",
       " 'hand',\n",
       " 'cyril',\n",
       " 'synod',\n",
       " 'ephesu',\n",
       " 'pope',\n",
       " 'council',\n",
       " 'chalcedon',\n",
       " 'weight',\n",
       " 'cast',\n",
       " 'eastern',\n",
       " 'empir',\n",
       " 'demand',\n",
       " 'notic',\n",
       " 'reader',\n",
       " 'amus',\n",
       " 'variou',\n",
       " 'prospect',\n",
       " 'nestorian',\n",
       " 'jacobit',\n",
       " 'maro',\n",
       " 'nite',\n",
       " 'armenian',\n",
       " 'copt',\n",
       " 'abyssinian',\n",
       " 'three',\n",
       " 'former',\n",
       " 'syriac',\n",
       " 'common',\n",
       " 'latter',\n",
       " 'discrimin',\n",
       " 'nation',\n",
       " 'idiom',\n",
       " 'modern',\n",
       " 'nativ',\n",
       " 'armenia',\n",
       " 'abyssinia',\n",
       " 'would',\n",
       " 'incapableof',\n",
       " 'convers',\n",
       " 'ancestor',\n",
       " 'christian',\n",
       " 'egypt',\n",
       " 'syria',\n",
       " 'reject',\n",
       " 'religion',\n",
       " 'adopt',\n",
       " 'languag',\n",
       " 'arabian',\n",
       " 'laps',\n",
       " 'time',\n",
       " 'second',\n",
       " 'sacerdot',\n",
       " 'east',\n",
       " 'well',\n",
       " 'west',\n",
       " 'deiti',\n",
       " 'address',\n",
       " 'obsolet',\n",
       " 'tongu',\n",
       " 'unknown',\n",
       " 'major',\n",
       " 'congreg',\n",
       " 'nativ',\n",
       " 'episcop',\n",
       " 'vinc',\n",
       " 'heresi',\n",
       " 'unfortun',\n",
       " 'nestoriu',\n",
       " 'speedili',\n",
       " 'obliter',\n",
       " 'orient',\n",
       " 'bishop',\n",
       " 'ephesu',\n",
       " 'resist',\n",
       " 'face',\n",
       " 'arrog',\n",
       " 'cyril',\n",
       " 'mollifi',\n",
       " 'tardi',\n",
       " 'concess',\n",
       " 'prelat',\n",
       " 'successor',\n",
       " 'subscrib',\n",
       " 'murmur',\n",
       " 'decre',\n",
       " 'chalcedon',\n",
       " 'power',\n",
       " 'monophysit',\n",
       " 'reconcil',\n",
       " 'catho',\n",
       " 'conform',\n",
       " 'passion',\n",
       " 'interest',\n",
       " 'insen',\n",
       " 'sibil',\n",
       " 'belief',\n",
       " 'last',\n",
       " 'reluct',\n",
       " 'sigh',\n",
       " 'breath',\n",
       " 'defenc',\n",
       " 'three',\n",
       " 'chapter',\n",
       " 'dissent',\n",
       " 'brethren',\n",
       " 'less',\n",
       " 'moder',\n",
       " 'sincerej',\n",
       " 'account',\n",
       " 'honophysit',\n",
       " 'nestorian',\n",
       " 'deepli',\n",
       " 'indebt',\n",
       " 'bibliotheca',\n",
       " 'orientau',\n",
       " 'clementin',\n",
       " 'vaticana',\n",
       " 'joseph',\n",
       " 'simon',\n",
       " 'assemannu',\n",
       " 'learn',\n",
       " 'maronit',\n",
       " 'dispatch',\n",
       " 'year',\n",
       " 'pope',\n",
       " 'clement',\n",
       " 'isit',\n",
       " 'monasteri',\n",
       " 'egypt',\n",
       " 'syiia',\n",
       " 'search',\n",
       " 'four',\n",
       " 'folio',\n",
       " 'volum',\n",
       " 'publish',\n",
       " 'ronv',\n",
       " 'contain',\n",
       " 'part',\n",
       " 'onli',\n",
       " 'though',\n",
       " 'perhap',\n",
       " 'valoabla',\n",
       " 'extens',\n",
       " 'project',\n",
       " 'nativ',\n",
       " 'scholar',\n",
       " 'possess',\n",
       " 'syriac',\n",
       " 'literatur',\n",
       " 'though',\n",
       " 'depend',\n",
       " 'rome',\n",
       " 'wish',\n",
       " 'moder',\n",
       " 'candid',\n",
       " 'roman',\n",
       " 'empir',\n",
       " 'fault',\n",
       " 'virtu',\n",
       " 'decid',\n",
       " 'whether',\n",
       " 'titl',\n",
       " 'enthusiast',\n",
       " 'impostor',\n",
       " 'properli',\n",
       " 'belong',\n",
       " 'extraordinari',\n",
       " 'intim',\n",
       " 'versant',\n",
       " 'abdallah',\n",
       " 'task',\n",
       " 'would',\n",
       " 'still',\n",
       " 'difficult',\n",
       " 'success',\n",
       " 'uncertain',\n",
       " 'distanc',\n",
       " 'twelv',\n",
       " 'centuri',\n",
       " 'darkli',\n",
       " 'contempl',\n",
       " 'shade',\n",
       " 'cloud',\n",
       " 'religi',\n",
       " 'incens',\n",
       " 'could',\n",
       " 'truli',\n",
       " 'delin',\n",
       " 'portrait',\n",
       " 'hour',\n",
       " 'fleet',\n",
       " 'resembl',\n",
       " 'would',\n",
       " 'equal',\n",
       " 'appli',\n",
       " 'solitari',\n",
       " 'mount',\n",
       " 'hera',\n",
       " 'preacher',\n",
       " 'mecca',\n",
       " 'conqueror',\n",
       " 'arabia',\n",
       " 'author',\n",
       " 'mighti',\n",
       " 'revolut',\n",
       " 'appear',\n",
       " 'endow',\n",
       " 'piou',\n",
       " 'contempl',\n",
       " 'disposit',\n",
       " 'soon',\n",
       " 'marriag',\n",
       " 'rais',\n",
       " 'abov',\n",
       " 'pressur',\n",
       " 'want',\n",
       " 'avoid',\n",
       " 'path',\n",
       " 'ambit',\n",
       " 'avaric',\n",
       " 'till',\n",
       " 'forti',\n",
       " 'live',\n",
       " 'innoc',\n",
       " 'would',\n",
       " 'without',\n",
       " 'name',\n",
       " 'uniti',\n",
       " 'idea',\n",
       " 'congeni',\n",
       " 'natur',\n",
       " 'reason',\n",
       " 'slight',\n",
       " 'convers',\n",
       " 'christian',\n",
       " 'would',\n",
       " 'teach',\n",
       " 'despis',\n",
       " 'detest',\n",
       " 'idolatri',\n",
       " 'mecca',\n",
       " 'duti',\n",
       " 'citizen',\n",
       " 'impart',\n",
       " 'doctrin',\n",
       " 'salvat',\n",
       " 'rescu',\n",
       " 'countri',\n",
       " 'minion',\n",
       " 'error',\n",
       " 'energi',\n",
       " 'mind',\n",
       " 'santli',\n",
       " 'bent',\n",
       " 'object',\n",
       " 'would',\n",
       " 'convert',\n",
       " 'gener',\n",
       " 'oblig',\n",
       " 'particular',\n",
       " 'call',\n",
       " 'warm',\n",
       " 'suggest',\n",
       " 'understand',\n",
       " 'fanci',\n",
       " 'would',\n",
       " 'felt',\n",
       " 'inspir',\n",
       " 'heaven',\n",
       " 'labour',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'pire',\n",
       " 'raptur',\n",
       " 'vision',\n",
       " 'inward',\n",
       " 'sensat',\n",
       " 'invis',\n",
       " 'monitor',\n",
       " 'would',\n",
       " 'describ',\n",
       " 'form',\n",
       " 'attribut',\n",
       " 'angel',\n",
       " 'enthusiasm',\n",
       " 'impostur',\n",
       " 'step',\n",
       " 'peril',\n",
       " 'slipperi',\n",
       " 'socratesr',\n",
       " 'afford',\n",
       " 'memor',\n",
       " 'instanc',\n",
       " 'christian',\n",
       " 'rashli',\n",
       " 'enough',\n",
       " 'assign',\n",
       " 'mahomet',\n",
       " 'tarn',\n",
       " 'pigeon',\n",
       " 'seem',\n",
       " 'descend',\n",
       " 'heaven',\n",
       " 'whisper',\n",
       " 'pretend',\n",
       " 'miracl',\n",
       " 'grotiu',\n",
       " 'verit',\n",
       " 'religion',\n",
       " 'christiana',\n",
       " 'arab',\n",
       " 'translat',\n",
       " 'learn',\n",
       " 'pocock',\n",
       " 'inquir',\n",
       " 'name',\n",
       " 'author',\n",
       " 'grotiu',\n",
       " 'confess',\n",
       " 'unknown',\n",
       " 'mahometan',\n",
       " 'themselv',\n",
       " 'lest',\n",
       " 'provok',\n",
       " 'indign',\n",
       " 'laughter',\n",
       " 'piou',\n",
       " 'suppress',\n",
       " 'arab',\n",
       " 'version',\n",
       " 'maintain',\n",
       " 'edifi',\n",
       " 'place',\n",
       " 'numer',\n",
       " 'edit',\n",
       " 'text',\n",
       " 'cock',\n",
       " 'specimen',\n",
       " 'hist',\n",
       " 'arabum',\n",
       " 'reland',\n",
       " 'religion',\n",
       " 'moharn',\n",
       " 'chap',\n",
       " 'xlix',\n",
       " 'signal',\n",
       " 'ravenna',\n",
       " 'venic',\n",
       " 'citi',\n",
       " 'exarch',\n",
       " 'pentapoli',\n",
       " 'adher',\n",
       " 'caus',\n",
       " 'religion',\n",
       " 'militari',\n",
       " 'forc',\n",
       " 'land',\n",
       " 'consist',\n",
       " 'part',\n",
       " 'nativ',\n",
       " 'spirit',\n",
       " 'patriot',\n",
       " 'zeal',\n",
       " 'transfus',\n",
       " 'mercenari',\n",
       " 'stranger',\n",
       " 'italian',\n",
       " 'swore',\n",
       " 'live',\n",
       " 'defenc',\n",
       " 'pope',\n",
       " 'holi',\n",
       " 'imag',\n",
       " 'roman',\n",
       " 'peopl',\n",
       " 'vote',\n",
       " 'father',\n",
       " 'even',\n",
       " 'lombard',\n",
       " 'bitiou',\n",
       " 'share',\n",
       " 'merit',\n",
       " 'advantag',\n",
       " 'holi',\n",
       " 'treason',\n",
       " 'obviou',\n",
       " 'reveng',\n",
       " 'destruct',\n",
       " 'statu',\n",
       " 'self',\n",
       " 'effectu',\n",
       " 'pleas',\n",
       " 'measur',\n",
       " 'rebel',\n",
       " 'lion',\n",
       " 'withhold',\n",
       " 'tribut',\n",
       " 'itali',\n",
       " 'prive',\n",
       " 'power',\n",
       " 'recent',\n",
       " 'abus',\n",
       " 'imposit',\n",
       " 'capit',\n",
       " 'form',\n",
       " 'ministr',\n",
       " 'preserv',\n",
       " 'elect',\n",
       " 'magistr',\n",
       " 'governor',\n",
       " 'high',\n",
       " 'public',\n",
       " 'indign',\n",
       " 'italian',\n",
       " 'prepar',\n",
       " 'creat',\n",
       " 'orthodox',\n",
       " 'emperor',\n",
       " 'conduct',\n",
       " 'fleet',\n",
       " 'armi',\n",
       " 'palac',\n",
       " 'constantinopl',\n",
       " 'palac',\n",
       " 'bishop',\n",
       " 'second',\n",
       " 'third',\n",
       " 'gregori',\n",
       " 'demn',\n",
       " 'author',\n",
       " 'revolt',\n",
       " 'everi',\n",
       " 'attempt',\n",
       " 'made',\n",
       " 'either',\n",
       " 'fraud',\n",
       " 'forc',\n",
       " 'seifc',\n",
       " 'strike',\n",
       " 'live',\n",
       " 'citi',\n",
       " 'repeat',\n",
       " 'edli',\n",
       " 'visit',\n",
       " 'assault',\n",
       " 'captain',\n",
       " 'guard',\n",
       " 'duke',\n",
       " 'exarch',\n",
       " 'high',\n",
       " 'digniti',\n",
       " 'secret',\n",
       " 'trust',\n",
       " 'land',\n",
       " 'foreign',\n",
       " 'troop',\n",
       " 'obtain',\n",
       " 'dome',\n",
       " 'superstit',\n",
       " 'napl',\n",
       " 'blush',\n",
       " 'father',\n",
       " 'attach',\n",
       " 'caus',\n",
       " 'heresi',\n",
       " 'clandestin',\n",
       " 'open',\n",
       " 'attack',\n",
       " 'repel',\n",
       " 'torem',\n",
       " 'quasi',\n",
       " 'contra',\n",
       " 'hmleni',\n",
       " 'armavit',\n",
       " 'renuen',\n",
       " 'haeresim',\n",
       " 'scriben',\n",
       " 'ubiqu',\n",
       " 'caver',\n",
       " 'christiana',\n",
       " 'quod',\n",
       " 'orta',\n",
       " 'fuisset',\n",
       " 'impieta',\n",
       " 'tali',\n",
       " 'igitur',\n",
       " 'permoti',\n",
       " 'pentapolens',\n",
       " 'atqu',\n",
       " 'venetiarum',\n",
       " 'exercitu',\n",
       " 'contra',\n",
       " 'imperatori',\n",
       " 'jussionem',\n",
       " 'restit',\n",
       " 'runt',\n",
       " 'dicent',\n",
       " 'nunquam',\n",
       " 'ejusdem',\n",
       " 'pontifici',\n",
       " 'condescender',\n",
       " 'necem',\n",
       " 'magi',\n",
       " 'defension',\n",
       " 'virilit',\n",
       " 'decertar',\n",
       " 'censu',\n",
       " 'capit',\n",
       " 'anastasiu',\n",
       " 'cruel',\n",
       " 'unknown',\n",
       " 'saracen',\n",
       " 'themselv',\n",
       " 'exclaim',\n",
       " 'zealou',\n",
       " 'maimbonrg',\n",
       " 'hisi',\n",
       " 'icono',\n",
       " 'clast',\n",
       " 'theophan',\n",
       " 'talk',\n",
       " 'pharaoh',\n",
       " 'number',\n",
       " 'male',\n",
       " 'children',\n",
       " 'israel',\n",
       " 'mode',\n",
       " 'taxat',\n",
       " 'familiar',\n",
       " 'saracen',\n",
       " 'unluckili',\n",
       " 'historian',\n",
       " 'impos',\n",
       " 'year',\n",
       " 'afterward',\n",
       " 'franc',\n",
       " 'patron',\n",
       " 'loui',\n",
       " 'chap',\n",
       " 'product',\n",
       " 'invent',\n",
       " 'familiar',\n",
       " 'present',\n",
       " 'numer',\n",
       " 'arab',\n",
       " 'indian',\n",
       " 'cipher',\n",
       " 'monli',\n",
       " 'style',\n",
       " 'regul',\n",
       " 'offic',\n",
       " 'promot',\n",
       " 'import',\n",
       " 'discoveri',\n",
       " 'arithmet',\n",
       " 'algebra',\n",
       " 'mathemat',\n",
       " 'scienc',\n",
       " 'second',\n",
       " 'whilst',\n",
       " 'caliph',\n",
       " 'wale',\n",
       " 'throne',\n",
       " 'damascu',\n",
       " 'lieuten',\n",
       " 'achiev',\n",
       " 'conquest',\n",
       " 'transoxiana',\n",
       " 'spain',\n",
       " 'third',\n",
       " 'armi',\n",
       " 'saracen',\n",
       " 'overspread',\n",
       " 'provinc',\n",
       " 'asia',\n",
       " 'minor',\n",
       " 'approach',\n",
       " 'border',\n",
       " 'byzantin',\n",
       " 'capit',\n",
       " 'attempt',\n",
       " 'disgrac',\n",
       " 'second',\n",
       " 'sieg',\n",
       " 'reserv',\n",
       " 'brother',\n",
       " 'soliman',\n",
       " 'whose',\n",
       " 'bition',\n",
       " 'appear',\n",
       " 'quicken',\n",
       " 'activ',\n",
       " 'martial',\n",
       " 'spirit',\n",
       " 'revolut',\n",
       " 'greek',\n",
       " 'pire',\n",
       " 'tyrant',\n",
       " 'justinian',\n",
       " 'punish',\n",
       " 'aveng',\n",
       " 'humbl',\n",
       " 'secretari',\n",
       " 'anastasiu',\n",
       " 'artemiu',\n",
       " 'promot',\n",
       " 'chanc',\n",
       " 'merit',\n",
       " 'vacant',\n",
       " 'purpl',\n",
       " 'alarm',\n",
       " 'sound',\n",
       " 'amba',\n",
       " 'sador',\n",
       " 'return',\n",
       " 'damascu',\n",
       " 'tremend',\n",
       " 'news',\n",
       " 'saracen',\n",
       " 'prepar',\n",
       " 'armament',\n",
       " 'land',\n",
       " 'would',\n",
       " 'transcend',\n",
       " 'experi',\n",
       " 'past',\n",
       " 'belief',\n",
       " 'present',\n",
       " 'caution',\n",
       " 'anastasiu',\n",
       " 'unworthi',\n",
       " 'station',\n",
       " 'impend',\n",
       " 'danger',\n",
       " 'issu',\n",
       " 'peremptori',\n",
       " 'mandat',\n",
       " 'person',\n",
       " 'provid',\n",
       " 'mean',\n",
       " 'subsist',\n",
       " 'three',\n",
       " 'year',\n",
       " 'sieg',\n",
       " 'evacu',\n",
       " 'citi',\n",
       " 'public',\n",
       " 'granari',\n",
       " 'arsen',\n",
       " 'abundantli',\n",
       " 'replenish',\n",
       " 'wall',\n",
       " 'restor',\n",
       " 'strengthen',\n",
       " 'engin',\n",
       " 'cast',\n",
       " ...]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teken_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_data = teken_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_data2 = teken_long2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_long = [word for word in teken_long if word not in eng_stopwords] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_long2 = [word for word in teken_long2 if word not in eng_stopwords] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_long = removeshort(token_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_long2 = removeshort(token_long2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer1 = TfidfVectorizer(\n",
    "                            max_df=1.0, max_features=None,\n",
    "                            min_df=1, stop_words='english'\n",
    "                            )\n",
    "# , tokenizer=tokenize_and_stem, ngram_range=(1,3)\n",
    "tfidf_books = tfidf_vectorizer1.fit_transform(token_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(tfidf_vectorizer1,'tfidf_vectorizer_max1.0_min1.0.pkl')\n",
    "save(tfidf_books,'tfidf_max1.0_min1.0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer2 = TfidfVectorizer(\n",
    "                            max_df=1.0, max_features=None,\n",
    "                            min_df=1, stop_words='english',ngram_range=(1,3)\n",
    "                            )\n",
    "# , tokenizer=tokenize_and_stem, ngram_range=(1,3)\n",
    "tfidf_books2 = tfidf_vectorizer2.fit_transform(token_long2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(tfidf_vectorizer2,'tfidf_vectorizer_max1_min1_ngram1-3_lemma.pkl')\n",
    "save(tfidf_books2,'tfidf_max1_min1_ngram1-3_lemma.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer3 = TfidfVectorizer(\n",
    "                            max_df=1.0, max_features=None,\n",
    "                            min_df=1, stop_words='english', ngram_range=(1,3)\n",
    "                            )\n",
    "# , tokenizer=tokenize_and_stem\n",
    "tfidf_books3 = tfidf_vectorizer3.fit_transform(token_long)\n",
    "save(tfidf_vectorizer3,'tfidf_vectorizer_max1.0_min1_ngram1-3.pkl')\n",
    "save(tfidf_books3,'tfidf_max1.0_min1.0_ngram1-3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer4 = TfidfVectorizer(\n",
    "                            max_df=1.0, max_features=None,\n",
    "                            min_df=1, stop_words='english'\n",
    "                            )\n",
    "# , tokenizer=tokenize_and_stem\n",
    "tfidf_books4 = tfidf_vectorizer4.fit_transform(token_long2)\n",
    "save(tfidf_vectorizer4,'tfidf_vectorizer_max1.0_min1_lemma.pkl')\n",
    "save(tfidf_books4,'tfidf_max1.0_min1.0_lemma.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-0daa60785ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# >>> model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtagged_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tagged_docs.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtagged_docs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_long2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_docs' is not defined"
     ]
    }
   ],
   "source": [
    "index = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "tagged_docs = []\n",
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "# >>> model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "tagged_docs.append(gensim.models.doc2vec.TaggedDocument(token_long, [str(index)]))\n",
    "save(tagged_docs,'tagged_docs.pkl')\n",
    "tagged_docs2.append(gensim.models.doc2vec.TaggedDocument(token_long2, [str(index)]))\n",
    "save(tagged_docs2,'tagged_docs2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Input: text list.\n",
    "# Return: doc2vec model\n",
    "# ============================================================================= \n",
    "def doc2vec(text_list, output_size=2, epoch=100):\n",
    "    i = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "    tagged_docs = []\n",
    "    ## prepare doc2vec input - list of taggedDocument\n",
    "#     for index,text in enumerate(text_list):\n",
    "#         docTokens = preProcess(text)   \n",
    "#         print(\"tagging:\"+str(index))\n",
    "#         tagged_docs.append(gensim.models.doc2vec.TaggedDocument(docTokens, [str(index)]))\n",
    "#     load\n",
    "#     tagged_docs = load('tagged_docs.pkl')\n",
    "    #save(tagged_docs,'tagged_docs_author.pkl')\n",
    "        \n",
    "    # setup configurations\n",
    "    d2vm = gensim.models.Doc2Vec(vector_size=output_size, min_count=0, alpha=0.025, min_alpha=0.025)\n",
    "#     d2vm.build_vocab(tagged_docs)\n",
    "    \n",
    "    print(\"Training doc2vec model..\")\n",
    "    # Train the doc2vec model\n",
    "    for epoch in range(epoch):    # number of epoch\n",
    "        print(\"training ep:\"+str(epoch))\n",
    "        d2vm.train(tagged_docs, total_examples=len(tagged_docs), epochs=1 )\n",
    "        # Change learning rate for next epoch (start with large num to speed up at first and then decrease to fine grain learning)\n",
    "        d2vm.alpha -= 0.002\n",
    "        d2vm.min_alpha = d2vm.alpha\n",
    "   # d2vm.train(tagged_docs, total_examples=len(tagged_docs), epochs=epoch )\n",
    "    print(\"Done training..\")\n",
    "    ##d2vm.save('doc2vec.model')\n",
    "    return d2vm\n",
    "\n",
    "def doc2vec_to_vectors(d2vm): \n",
    "    # Extract vectors from doc2vec model\n",
    "    feature_vectors = []\n",
    "    for i in range(0,len(d2vm.docvecs)) :\n",
    "        feature_vectors.append(d2vm.docvecs[i])\n",
    "    \n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-499603458816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##------ Doc2vec---------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md2vm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_long\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#d2vm.save('doc2vec_e100_size300.model')#save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbookFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec_to_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2vm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-300-50fd13b0f712>\u001b[0m in \u001b[0;36mdoc2vec\u001b[0;34m(text_list, output_size, epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# setup configurations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0md2vm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#     d2vm.build_vocab(tagged_docs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "##------ Doc2vec---------\n",
    "d2vm = doc2vec(token_long,output_size=300,epoch=100)\n",
    "#d2vm.save('doc2vec_e100_size300.model')#save \n",
    "bookFeatures = doc2vec_to_vectors(d2vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Reduce dimensions\n",
    "# Input: distance of each vectors as matrix\n",
    "# Return: 2 dim vectors x[],y[]\n",
    "# =============================================================================\n",
    "def multidimScale(distanceVectors,mode=\"precomputed\"):\n",
    "    # multidimension scaling - metric:True=Mds, False=Nmds\n",
    "    if mode != \"precomputed\":\n",
    "        mode = \"euclidean\"\n",
    "    mds = MDS(n_components=2, random_state=1,dissimilarity=mode, metric=True)\n",
    "    pos = mds.fit_transform(distanceVectors)  # shape (n_components, n_samples)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    lowDimVecs = pd.DataFrame({'dim1':xs, 'dim2':ys})\n",
    "    return lowDimVecs\n",
    "def multidimScale3(distanceVectors,mode=\"precomputed\"):\n",
    "    # multidimension scaling - metric:True=Mds, False=Nmds\n",
    "    if mode != \"precomputed\":\n",
    "        mode = \"euclidean\"\n",
    "    mds = MDS(n_components=3, random_state=1,dissimilarity=mode, metric=True)\n",
    "    pos = mds.fit_transform(distanceVectors)  # shape (n_components, n_samples)\n",
    "    xs, ys, zs = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "    lowDimVecs = pd.DataFrame({'dim1':xs, 'dim2':ys, 'dim3':zs})\n",
    "    return lowDimVecs\n",
    "\n",
    "def tsne(featureVector,m = \"precomputed\"):\n",
    "    ts = TSNE(n_components=2, random_state=1, metric=m)\n",
    "    pos = ts.fit_transform(featureVector)  # shape (n_components, n_samples)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    lowDimVecs = pd.DataFrame({'dim1':xs, 'dim2':ys})\n",
    "    return lowDimVecs\n",
    "\n",
    "def svd(featureVector):\n",
    "    sv = TruncatedSVD(n_components=2, random_state=1)\n",
    "    pos = sv.fit_transform(bookFeatures)  \n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    lowDimVecs = pd.DataFrame({'dim1':xs, 'dim2':ys})\n",
    "    return lowDimVecs\n",
    "def pca():\n",
    "    return  0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Clustering\n",
    "# Input: feature vectors\n",
    "# Return: km objects\n",
    "# =============================================================================\n",
    "def kmean(featureVectors,k=6):    \n",
    "    km = KMeans(n_clusters=k,random_state =1)\n",
    "    km.fit(featureVectors)\n",
    "    return km\n",
    "\n",
    "\n",
    "def hc():\n",
    "    linkage_matrix = ward(distVectors) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "    \n",
    "    fig, ax = plt.subplots() # set size\n",
    "    ax = dendrogram(linkage_matrix, orientation=\"right\", labels=\"title\");\n",
    "    \n",
    "    plt.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    \n",
    "    plt.tight_layout() #show plot with tight layout\n",
    "    \n",
    "    #uncomment below to save figure\n",
    "    #plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1--save(tfidf_vectorizer1,'tfidf_vectorizer_max1.0_min1.0.pkl')\n",
    "# 1--save(tfidf_books,'tfidf_max1.0_min1.0.pkl')\n",
    "\n",
    "# 2--save(tfidf_vectorizer2,'tfidf_vectorizer_max1_min1_ngram1-3_lemma.pkl')\n",
    "# 2--save(tfidf_books2,'tfidf_max1_min1_ngram1-3_lemma.pkl')\n",
    "\n",
    "# 3--save(tfidf_vectorizer3,'tfidf_vectorizer_max1.0_min1_ngram1-3.pkl')\n",
    "# 3--save(tfidf_books3,'tfidf_max1.0_min1.0_ngram1-3.pkl')\n",
    "\n",
    "# 4--save(tfidf_vectorizer4,'tfidf_vectorizer_max1.0_min1_lemma.pkl')\n",
    "# 4--save(tfidf_books4,'tfidf_max1.0_min1.0_lemma.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##-- Load obj from file    \n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as input: \n",
    "        obj = pickle.load(input)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0749c47267ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbookFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf_max1.0_min1.0.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdistVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbookFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdissimilarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"precomputed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbookFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bookFeatures = load('tfidf_max1.0_min1.0.pkl')\n",
    "distVectors = 1-cosine_similarity(bookFeatures)\n",
    "mds = MDS(n_components=2000, random_state=1,dissimilarity=\"precomputed\", metric=True)\n",
    "bookFeatures = mds.fit_transform(distVectors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bookFeatures2 = load('tfidf_max1_min1_ngram1-3_lemma.pkl')\n",
    "distVectors2 = 1-cosine_similarity(bookFeatures2)\n",
    "mds2 = MDS(n_components=2000, random_state=1,dissimilarity=\"precomputed\", metric=True)\n",
    "bookFeatures2 = mds2.fit_transform(distVectors2) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookFeatures3 = load('tfidf_max1.0_min1.0_ngram1-3.pkl')\n",
    "distVectors3 = 1-cosine_similarity(bookFeatures3)\n",
    "mds3 = MDS(n_components=2000, random_state=1,dissimilarity=\"precomputed\", metric=True)\n",
    "bookFeatures3 = mds3.fit_transform(distVectors3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookFeatures4 = load('tfidf_max1.0_min1.0_lemma.pkl')\n",
    "distVectors4 = 1-cosine_similarity(bookFeatures4)\n",
    "mds4 = MDS(n_components=2000, random_state=1,dissimilarity=\"precomputed\", metric=True)\n",
    "bookFeatures4 = mds4.fit_transform(distVectors4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# k means determine k\n",
    "# =============================================================================\n",
    "\n",
    "##Process\n",
    "print (\"Clustering books..\")\n",
    "km = kmean(bookFeatures,6)\n",
    "clusters = km.labels_.tolist()\n",
    "bookFeatures_with_c = np.vstack([bookFeatures, km.cluster_centers_])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
